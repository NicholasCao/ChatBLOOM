# ChatBloom
## QuickStart
Chat with our trained model:
```bash
python chat.py
```

## Training
### Install
```bash
conda create -n coati
conda activate coati
pip install .

cd ..
git clone https://github.com/hpcaitech/transformers
cd transformers
pip install .

cd ..
git clone git@github.com:hpcaitech/ColossalAI.git
cd ColossalAI
git checkout e6a132a
pip install .
```

### Instruction Tuning (Optional)
> This stage is optional, because we can use [bloomz](https://huggingface.co/bigscience/bloomz-1b7) directly.

Data:
<!-- - [pCLUE](https://huggingface.co/datasets/wbbbbb/pclue) | [github](https://github.com/CLUEbenchmark/pCLUE)
- [BELLE Generated Chat](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)
- [BELLE train_2M_CN](https://huggingface.co/datasets/BelleGroup/train_2M_CN) -->

|Dataset | Size | Used |
| - | - | - |
| [pCLUE](https://huggingface.co/datasets/wbbbbb/pclue) | 1.2M | 0.3M |
| [BELLE Generated Chat](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M) | 0.4M | 0.2M |
| [BELLE train_2M_CN](https://huggingface.co/datasets/BelleGroup/train_2M_CN) | 2M | 0.5M |


Run:
```bash
bash scripts/train_instruction_tuning.sh
```

### RLHF - Stage 1 - SFT

Data:

|Dataset | Size | Used |
| - | - | - |
| [BELLE/1M](https://huggingface.co/datasets/BelleGroup/train_1M_CN) | 1M | 40k |
| [BELLE Multiturn Chat](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M) | 0.8M | 200K |
| [InstructionWild](https://github.com/XueFuzhao/InstructionWild) | 52k * 2 | 30k * 2 |

format as
```
Human: [Instruction or Input]
Assistant: [Output]
```

Run:
```bash
bash scripts/train_sft.sh
```

### RLHF - Stage 2 - RM
|Dataset | Size | Used |
| - | - | - |
| [BELLE/1M](https://huggingface.co/datasets/BelleGroup/train_1M_CN) | 1M | 10k |
| [BELLE Multiturn Chat](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M) | 0.8M | 20k |
| [InstructionWild](https://github.com/XueFuzhao/InstructionWild) | 52k * 2 | 10k * 2 |
| [Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf) | 161k | 10k |
> Note that this step uses data that does not overlap with the SFT.

Training the RM model lacks corresponding Chinese data, we simply treat the data generated by our SFT model as "rejected" and the data generated by ChatGPT as "chosen". 
1. Run the following script to generate the data:
```bash
python generate_rm_dataset.py
```
2. Train the RM model:
```bash
bash scripts/train_rm.sh
```

### RLHF - Stage 3 - PPO-RL 
```bash
bash scripts/train_ppo.sh
```

## Limitation and Usage Limits
The datasets we used (e.g. [BELLE](https://github.com/LianjiaTech/BELLE)) require developers only use the data for research purposes. Thus, commercial and other potentially harmful uses of our models are not allowed.

## Acknowledgements
This project is based on [ColossalAI](https://github.com/hpcaitech/ColossalAI) and [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat), and thanks to these projects for their contributions to open source.


