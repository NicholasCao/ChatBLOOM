# ChatBLOOM
## QuickStart
Chat with our trained model:
```bash
python chat.py
```

## Training
### Install
```bash
conda create -n coati
conda activate coati
pip install .

cd ..
git clone https://github.com/hpcaitech/transformers
cd transformers
pip install .

cd ..
git clone git@github.com:hpcaitech/ColossalAI.git
cd ColossalAI
git checkout e6a132a
pip install .
```

### Instruction Tuning (Optional)
> This stage is optional, because we can use [bloomz](https://huggingface.co/bigscience/bloomz-1b7) directly.

Data:

|Dataset | Size | Used |
| - | - | - |
| [pCLUE](https://huggingface.co/datasets/wbbbbb/pclue) | 1.2M | 0.3M |
| [BELLE Generated Chat](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M) | 0.4M | 0.2M |
| [BELLE train_2M_CN](https://huggingface.co/datasets/BelleGroup/train_2M_CN) | 2M | 0.5M |


Run:
```bash
bash scripts/train_instruction_tuning.sh
```

### RLHF - Stage 1 - SFT

#### Data:

|Dataset | Size | Used |
| - | - | - |
| [BELLE/1M](https://huggingface.co/datasets/BelleGroup/train_1M_CN) | 1M | 80K |
| [BELLE Multiturn Chat](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M) | 0.8M | 100K |
| [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) | 52k * 2 | 100K |
| [ShareGPT](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna) | 120k | 120K |

The dataset format as:
```
<Human>: {query} <eoh>
<Assistant>: {response} <eoa>
```
which can be generated by:
```bash
python generate_sft_dataset.py
```

#### Run
```bash
bash scripts/train_sft.sh
```

### RLHF - Stage 2 - RM
|Dataset | Size | Used |
| - | - | - |
| [BELLE/1M](https://huggingface.co/datasets/BelleGroup/train_1M_CN) | 1M | 20k |
| [BELLE Multiturn Chat](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M) | 0.8M | 20k |
| [InstructionWild](https://github.com/XueFuzhao/InstructionWild) | 52k * 2 | 10k |
| [Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf) | 161k | 10k |
> Note that this step uses data that does not overlap with the SFT.

Training the RM model lacks corresponding Chinese data, we simply treat the data generated by our SFT model as "rejected" and the data generated by ChatGPT as "chosen". 
1. Run the following script to generate the data (this step also generates the prompt dataset for the PPO):
```bash
python generate_rm_dataset.py
```
2. Train the RM model:
```bash
bash scripts/train_rm.sh
```

### RLHF - Stage 3 - PPO-RL
We run ppo failed based on ColossalChat, rewrite it based on [trl](https://github.com/lvwerra/trl).
```bash
bash scripts/train_ppo.sh
```

## Limitation and Usage Limits
The datasets we used (e.g. [BELLE](https://github.com/LianjiaTech/BELLE)) require developers only use the data for research purposes. Thus, commercial and other potentially harmful uses of our models are not allowed.

## Acknowledgements
This project is based on [ColossalAI](https://github.com/hpcaitech/ColossalAI), [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat) and [trl](https://github.com/lvwerra/trl), and thanks to these projects for their contributions to open source.
